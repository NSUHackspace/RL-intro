{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"p75_policy_iteration.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMZDpez7p9rSW+ttnNhhwfC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Алгоритм решения МППР итерацией по стратегиям состоит из 2х шагов: оценка стратегии и её улучшение. Первоначальную стратегию выбираем произвольно. Затем итеративно оцениваем её до сходимости с помощью уравнения мат ожидания Беллмана: \n","\n","$V(s) = \\sum_{s'} T(s,a,s') \\left[ R(s,a) + \\gamma  V(s') \\right]$\n","\n","Уравнение похоже на то, что мы использовали ранее, просто теперь вычисление вознаграждения осуществляется как мат ожидание по всем состояниям для действия $a = \\pi(s)$, которое возвращает стратегия."],"metadata":{"id":"sbXkJv6gSsyZ"}},{"cell_type":"markdown","source":["На основе полученной фукции ценности $V(s)$ улучшаем стратегию с помощь уравнения оптимальности\n","\n","$\\pi(s) = argmax_a \\sum_{s'}T(s,a,s')\\left[R(s,a,s') + \\gamma V(s')\\right]$\n","\n","Шаги повторяются, пока стратегия не сойдется. Стратегия и функция ценности, соответствующие неподвижной точке являются оптимальными."],"metadata":{"id":"PNrKbBzcVrc8"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"d5Am9D1SSjA6","executionInfo":{"status":"ok","timestamp":1654357159538,"user_tz":-420,"elapsed":4288,"user":{"displayName":"Hackspace Nsu","userId":"13714781562665069365"}}},"outputs":[],"source":["import torch\n","import gym\n","\n","env = gym.make('FrozenLake-v0')\n","\n","gamma = 0.99\n","\n","threshold = 0.0001"]},{"cell_type":"markdown","source":["Обращу внимание на то, что данный алгоритм предполагает два этапа итерации до сходимости: внутренний - при оценке текущей стратегии, и внешний - при улучшении стратегии."],"metadata":{"id":"64H8Uz88XBaG"}},{"cell_type":"code","source":["def policy_evaluation(env, policy, gamma, threshold):\n","    \"\"\"\n","    Perform policy evaluation\n","    @param env: OpenAI Gym environment\n","    @param policy: policy matrix containing actions and their probability in each state\n","    @param gamma: discount factor\n","    @param threshold: the evaluation will stop once values for all states are less than the threshold\n","    @return: values of the given policy\n","    \"\"\"\n","    n_state = policy.shape[0]\n","    V = torch.zeros(n_state)\n","    while True:\n","        V_temp = torch.zeros(n_state)\n","        for state in range(n_state):\n","            action = policy[state].item()\n","            for trans_prob, new_state, reward, _ in env.env.P[state][action]:\n","                V_temp[state] += trans_prob * (reward + gamma * V[new_state])\n","        max_delta = torch.max(torch.abs(V - V_temp))\n","        V = V_temp.clone()\n","        if max_delta <= threshold:\n","            break\n","    return V"],"metadata":{"id":"ltz-KP2XW780","executionInfo":{"status":"ok","timestamp":1654357163416,"user_tz":-420,"elapsed":952,"user":{"displayName":"Hackspace Nsu","userId":"13714781562665069365"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def policy_improvement(env, V, gamma):\n","    \"\"\"\n","    Obtain an improved policy based on the values\n","    @param env: OpenAI Gym environment\n","    @param V: policy values\n","    @param gamma: discount factor\n","    @return: the policy\n","    \"\"\"\n","    n_state = env.observation_space.n\n","    n_action = env.action_space.n\n","    policy = torch.zeros(n_state)\n","    for state in range(n_state):\n","        v_actions = torch.zeros(n_action)\n","        for action in range(n_action):\n","            for trans_prob, new_state, reward, _ in env.env.P[state][action]:\n","                v_actions[action] += trans_prob * (reward + gamma * V[new_state])\n","        policy[state] = torch.argmax(v_actions)\n","    return policy\n"],"metadata":{"id":"R753pBIYW-Cb","executionInfo":{"status":"ok","timestamp":1654357165855,"user_tz":-420,"elapsed":490,"user":{"displayName":"Hackspace Nsu","userId":"13714781562665069365"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def policy_iteration(env, gamma, threshold):\n","    \"\"\"\n","    Solve a given environment with policy iteration algorithm\n","    @param env: OpenAI Gym environment\n","    @param gamma: discount factor\n","    @param threshold: the evaluation will stop once values for all states are less than the threshold\n","    @return: optimal values and the optimal policy for the given environment\n","    \"\"\"\n","    n_state = env.observation_space.n\n","    n_action = env.action_space.n\n","    policy = torch.randint(high=n_action, size=(n_state,)).float()\n","    while True:\n","        V = policy_evaluation(env, policy, gamma, threshold)\n","        policy_improved = policy_improvement(env, V, gamma)\n","        if torch.equal(policy_improved, policy):\n","            return V, policy_improved\n","        policy = policy_improved"],"metadata":{"id":"CqatYKMCW_v3","executionInfo":{"status":"ok","timestamp":1654357168771,"user_tz":-420,"elapsed":481,"user":{"displayName":"Hackspace Nsu","userId":"13714781562665069365"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["V_optimal, optimal_policy = policy_iteration(env, gamma, threshold)\n","print('Optimal values:\\n{}'.format(V_optimal))\n","print('Optimal policy:\\n{}'.format(optimal_policy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wkEI9jirXqQy","executionInfo":{"status":"ok","timestamp":1654357170943,"user_tz":-420,"elapsed":697,"user":{"displayName":"Hackspace Nsu","userId":"13714781562665069365"}},"outputId":"fa7dead2-ecb0-4f5a-c080-296c3891142f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal values:\n","tensor([0.5404, 0.4966, 0.4681, 0.4541, 0.5569, 0.0000, 0.3572, 0.0000, 0.5905,\n","        0.6421, 0.6144, 0.0000, 0.0000, 0.7410, 0.8625, 0.0000])\n","Optimal policy:\n","tensor([0., 3., 3., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.])\n"]}]},{"cell_type":"markdown","source":["При выборе схемы поиска оптимальной стратегии для конкретной задачи будем пользоваться следующими соображениями: если действий немного, то итерация по ценности будет работать хорошо, для большого количества действий скорость поиска стратегии будет в среднем выше у итерации по стратегиям. Если есть неплохая стартовая стратегия, полученная из каких-то соображений, то можно начинать прямо с неё и итерировать по стратегиям."],"metadata":{"id":"bBfnRvNCX3iR"}},{"cell_type":"markdown","source":["К настоящему моменту, я надеюсь, что вам удалось разобраться со следующими понятиями: \n","функция ценности, стратегия, оценка стратегии, алгоритмы поиска оптимальной стратегии итерацией итерацией по ценности и итерацией по стратегиям. Это важные инструменты, которые понадобятся нам в будущем, чтобы строить сложные и эффективные алгоритмы."],"metadata":{"id":"D-1zrbi5YxY7"}}]}