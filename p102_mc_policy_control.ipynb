{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"p102_on_policy_mc_control.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMN5RFqoA7i8r1dGrZYFAb1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Вспомним нашу цель: найти оптимальную стратегию для агента. В мире, где доступна полная информация, мы могли воспользоваться итерацией по стратегиям, чтобы отыскать стратегию с наибольшим выигрышем. Условия метода Монте-Карло не позволят применить эту схему напрямую, потому что у нас нет информации о матрице переходовв T. \n"],"metadata":{"id":"wFHjBDcpaNsG"}},{"cell_type":"markdown","source":["В такой ситуации используются 2 схемы управления поиском стратегии: \n","1) Методы с единой стратегией (on-policy) обучаются оптимальной стратегии, сразу же исполняя её, а затем оценивая и улучшая.\n","2) Методы с разделенной стратегией (off-policy) генерируют данные с помощью одной стратегии, а затем обучают с их помощью целевую стратегию агента.\n","\n","\n","\n"],"metadata":{"id":"sEFu0lhHN73a"}},{"cell_type":"markdown","source":["Метод с единой стратегией -- частный случай метода с разделенной стратегий (когда поведенческая и целевая стратегии совпадают)"],"metadata":{"id":"ORVD0lL_UM3n"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6Iv-gW0OajR"},"outputs":[],"source":[""]}]}