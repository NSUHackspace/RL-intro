{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"p71_value_iteration.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNFCRugUWbUT9w4NUw5vAJN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Продолжаем искать решения МППР. В прошлой серии стало понятно, что для этого необходимо вычислить оптимальную стратегию. Для этого применяются 2 основных алгоритма: итерации по ценности и итерации по стратегиям."],"metadata":{"id":"bOd0faCT1QRC"}},{"cell_type":"markdown","source":["Сперва разберем итерацию по ценности. На старте у нас нет никакой стратегии и неизвестна функция ценности состояний. Что ж, инциализируем функцию ценности $V^*$ случайными значениями и модицифируем уравнение Беллмана так, чтобы вместо распределения, которое раньше задавалось стратегией $\\pi$, происходил жадный выбор действия, дающего наибольшую награду.\n","\n","$V^*(s) = max_a \\left[ R(s,a) + \\gamma \\sum_{s'} T(s,a,s') V^*(s') \\right]$\n","\n","в уравнении $V^*(s)$ -- оптимальная ценность состояния $s$ (она же ценность при следовании оптимальной стратегии), $T(s, a, s')$ -- вероятность перехода из состояния $s$ в $s'$ при выборе действия $a$ и $R(s,a)$ -- вознаграждение, полученное в состоянии $s$ при выборе действия $a$."],"metadata":{"id":"SL9h_rm428t0"}},{"cell_type":"markdown","source":["С помощью $V^*$ вычисляем оптимальную стратегию, снова выбирая действие так, чтобы оно максимизировало выигрыш\n","\n","$\\pi^*(s) = argmax_a \\sum_{s'}T(s,a,s')\\left[R(s,a,s') + \\gamma V^*(s')\\right]$"],"metadata":{"id":"qQA2Kpqr6sP0"}},{"cell_type":"markdown","source":["Посмотрим как работает описанный алгоритм в среде FrozenLake"],"metadata":{"id":"lM3Mnjld7xrq"}},{"cell_type":"markdown","source":["Среда FrozenLake представляет собой сетку по которой агент перемещается из клетки S в клетку G по замерзшим участкам озера (клетки F) и не попасть в полынью H. Игровой цикл на оптимальной стратегии можно посмотреть, добавив отрисовку поля."],"metadata":{"id":"SL3nix_PALZt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"t8BBzJIQxALg"},"outputs":[],"source":["import torch\n","import gym\n","\n","env = gym.make('FrozenLake-v0')\n","\n","gamma = 0.99\n","\n","threshold = 0.0001"]},{"cell_type":"code","source":["def value_iteration(env, gamma, threshold):\n","    \"\"\"\n","    Solve a given environment with value iteration algorithm\n","    @param env: OpenAI Gym environment\n","    @param gamma: discount factor\n","    @param threshold: the evaluation will stop once values for all states are less than the threshold\n","    @return: values of the optimal policy for the given environment\n","    \"\"\"\n","    n_state = env.observation_space.n\n","    n_action = env.action_space.n\n","    V = torch.zeros(n_state)\n","    while True:\n","        V_temp = torch.empty(n_state)\n","        for state in range(n_state):\n","            v_actions = torch.zeros(n_action)\n","            for action in range(n_action):\n","                for trans_prob, new_state, reward, _ in env.env.P[state][action]:\n","                    v_actions[action] += trans_prob * (reward + gamma * V[new_state])\n","            V_temp[state] = torch.max(v_actions)\n","        max_delta = torch.max(torch.abs(V - V_temp))\n","        V = V_temp.clone()\n","        if max_delta <= threshold:\n","            break\n","    return V"],"metadata":{"id":"Wqu3SZ3S74-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_optimal_policy(env, V_optimal, gamma):\n","    \"\"\"\n","    Obtain the optimal policy based on the optimal values\n","    @param env: OpenAI Gym environment\n","    @param V_optimal: optimal values\n","    @param gamma: discount factor\n","    @return: optimal policy\n","    \"\"\"\n","    n_state = env.observation_space.n\n","    n_action = env.action_space.n\n","    optimal_policy = torch.zeros(n_state)\n","    for state in range(n_state):\n","        v_actions = torch.zeros(n_action)\n","        for action in range(n_action):\n","            for trans_prob, new_state, reward, _ in env.env.P[state][action]:\n","                v_actions[action] += trans_prob * (reward + gamma * V_optimal[new_state])\n","        optimal_policy[state] = torch.argmax(v_actions)\n","    return optimal_policy"],"metadata":{"id":"xYVwhvmJ77LG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["V_optimal = value_iteration(env, gamma, threshold)\n","print('Optimal values:\\n{}'.format(V_optimal))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hvvQRILd79Dj","executionInfo":{"status":"ok","timestamp":1654350936278,"user_tz":-420,"elapsed":780,"user":{"displayName":"Hackspace Nsu","userId":"13714781562665069365"}},"outputId":"bb70e620-a067-4cc2-c9f4-970b85628397"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal values:\n","tensor([0.5404, 0.4966, 0.4681, 0.4541, 0.5569, 0.0000, 0.3572, 0.0000, 0.5905,\n","        0.6421, 0.6144, 0.0000, 0.0000, 0.7410, 0.8625, 0.0000])\n"]}]},{"cell_type":"code","source":["optimal_policy = extract_optimal_policy(env, V_optimal, gamma)\n","print('Optimal policy:\\n{}'.format(optimal_policy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"71tl6IAF7_l8","executionInfo":{"status":"ok","timestamp":1654350936280,"user_tz":-420,"elapsed":17,"user":{"displayName":"Hackspace Nsu","userId":"13714781562665069365"}},"outputId":"1f412af6-951e-405f-d97f-1ab59e990c3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal policy:\n","tensor([0., 3., 3., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.])\n"]}]},{"cell_type":"code","source":["def run_episode(env, policy):\n","    state = env.reset()\n","    total_reward = 0\n","    is_done = False\n","    while not is_done:\n","        action = policy[state].item()        \n","        state, reward, is_done, info = env.step(action)\n","        total_reward += reward\n","        if is_done:\n","            break\n","    return total_reward"],"metadata":{"id":"RtjvfL2q8BnT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_episode = 1000\n","total_rewards = []\n","for episode in range(n_episode):\n","    total_reward = run_episode(env, optimal_policy)\n","    total_rewards.append(total_reward)\n","\n","print('Average total reward under the optimal policy: {}'.format(sum(total_rewards) / n_episode))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HvkpZ3AO8vTq","executionInfo":{"status":"ok","timestamp":1654350949424,"user_tz":-420,"elapsed":1012,"user":{"displayName":"Hackspace Nsu","userId":"13714781562665069365"}},"outputId":"30375a1b-3b3c-4ff1-f39e-2678943e97ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average total reward under the optimal policy: 0.746\n"]}]},{"cell_type":"code","source":["def run_episode_render(env, policy):\n","    state = env.reset()\n","    env.render()\n","    total_reward = 0\n","    is_done = False\n","    while not is_done:\n","        action = int(policy[state].item())\n","        state, reward, is_done, info = env.step(action)\n","        total_reward += reward\n","        env.render()\n","        if is_done:\n","            break\n","    return total_reward"],"metadata":{"id":"TRv-CcTo8DiO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["run_episode_render(env, optimal_policy)"],"metadata":{"id":"GRKngqOp9xfc"},"execution_count":null,"outputs":[]}]}