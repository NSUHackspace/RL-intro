{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"p92_mc_policy_evaluation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNNGTFYABU/PzLrcxijWQ0H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["В прошлых сериях чтобы обучить агента мы собирали максимум информации о среде, проходя по всем доступным состояниям с помощью матрицы переходов $T$. Сбор этой информации обходится дорого даже если ограничивать себя стратегией $\\pi$. Хуже того, матрица переходов не всегда известна заранее! Чтобы преодолеть эти трудности нужен новый подход. \n","\n","Замечание. Алгоритмы, требующие для работы информацию об окружающей среде, называются **основанными на модели**, а алгоритмы не нуждающиеся в априорной информации о переходах и вознаграждениях называются **безмодельными**"],"metadata":{"id":"8ZdcXNdY1b8_"}},{"cell_type":"markdown","source":["Рассмотрим безмодельный метод Монте-Карло, основанный на рандомизации данных: мы строим выборку, анализируем её свойства и делам выводы о распределении, из которого она была извлечена. Чем больше итераций, тем точнее приближается искомое распределение. "],"metadata":{"id":"mwgle40K4jip"}},{"cell_type":"markdown","source":["Итак, у нас есть игра, но нет данных о матрице переходов и вознаграждений. Полное вознаграждение (или доход), полученное в течение эпизода, начиная с момента времени t, выражается формулой\n","\n","$G_t = \\sum_k \\gamma^k R_{t+k+1}$\n","\n","Но теперь его оценка имеет эмпирический характер, т.к. строится на основе множества выборок. \n","\n","Замечание: $G_t$ удобнее вычислять от конца к началу. И это в общем-то достаточно интуитивно, если помнить, что занимаемся планированием и считаем шаги наперёд. "],"metadata":{"id":"HZ1R03vA6OXd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UOcPE2cy1Ym7"},"outputs":[],"source":["import torch\n","import gym\n","\n","env = gym.make('FrozenLake-v0')\n","\n","gamma = 1\n","n_episode = 10000\n","\n","# оптимальная стратегия для игры FrozenLake, вычисленная в предыдущих сериях\n","optimal_policy = torch.tensor([0., 3., 3., 3., 0., 3., 2., 3., 3., 1., 0., 3., 3., 2., 1., 3.])"]},{"cell_type":"code","source":["def run_episode(env, policy):\n","    state = env.reset()\n","    rewards = []\n","    states = [state]\n","    is_done = False\n","    while not is_done:\n","        action = policy[state].item()\n","        state, reward, is_done, info = env.step(action)\n","        states.append(state)\n","        rewards.append(reward)\n","        if is_done:\n","            break\n","    states = torch.tensor(states)\n","    rewards = torch.tensor(rewards)\n","    return states, rewards"],"metadata":{"id":"cv-5kB0M8TaY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Дополнительно позникает интересный вопрос: если состояние s в течение эпизода посещается несколько раз, сколько раз стоит учитывать его при вычислении функции ценности? Основных подходов два: учесть самое первое посещение и усреднить по всем посещениям в рамках эпизода. Рассмотрим, как считается каждый из вариантов:"],"metadata":{"id":"nivWj-pR9XfQ"}},{"cell_type":"code","source":["def mc_prediction_first_visit(env, policy, gamma, n_episode):\n","    n_state = policy.shape[0]\n","    V = torch.zeros(n_state)\n","    N = torch.zeros(n_state)\n","    for episode in range(n_episode):\n","        states_t, rewards_t = run_episode(env, policy)\n","        return_t = 0\n","        first_visit = torch.zeros(n_state)\n","        G = torch.zeros(n_state)\n","        for state_t, reward_t in zip(reversed(states_t)[1:], reversed(rewards_t)):\n","            return_t = gamma * return_t + reward_t\n","            G[state_t] = return_t\n","            first_visit[state_t] = 1\n","        for state in range(n_state):\n","            if first_visit[state] > 0:\n","                V[state] += G[state]\n","                N[state] += 1\n","    for state in range(n_state):\n","        if N[state] > 0:\n","            V[state] = V[state] / N[state]\n","    return V"],"metadata":{"id":"_k4-B4NG8U4I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["value = mc_prediction_first_visit(env, optimal_policy, gamma, n_episode)\n","\n","print('The value function calculated by first-visit MC prediction:\\n', value)"],"metadata":{"id":"Dmew9uMj8jGP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mc_prediction_every_visit(env, policy, gamma, n_episode):\n","    n_state = policy.shape[0]\n","    V = torch.zeros(n_state)\n","    N = torch.zeros(n_state)\n","    G = torch.zeros(n_state)\n","    for episode in range(n_episode):\n","        states_t, rewards_t = run_episode(env, policy)\n","        return_t = 0\n","        for state_t, reward_t in zip(reversed(states_t)[1:], reversed(rewards_t)):\n","            return_t = gamma * return_t + reward_t\n","            G[state_t] += return_t\n","            N[state_t] += 1\n","    for state in range(n_state):\n","        if N[state] > 0:\n","            V[state] = G[state] / N[state]\n","    return V"],"metadata":{"id":"V7aL1ulE9awJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["value = mc_prediction_every_visit(env, optimal_policy, gamma, n_episode)\n","\n","print('The value function calculated by every-visit MC prediction:\\n', value)"],"metadata":{"id":"GdlZb9XC9b4b"},"execution_count":null,"outputs":[]}]}